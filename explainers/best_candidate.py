from typing import TYPE_CHECKING

import numpy as np
from tqdm.auto import tqdm

from detectors.random_forest import RandomForest
from explainers.explainer import Explainer
from utilities.metrics import dist_euclidean
from utilities.tree_tools import TreeContraster

if TYPE_CHECKING:
    from manager import MethodManager


class AFT(Explainer):
    '''
    An implementation of the Actionable Feature Tweaking method developed in "Interpretable Predictions of Tree-based Ensembles via Actionable Feature Tweaking." The original paper can be found at https://arxiv.org/abs/1706.06691
    '''

    def __init__(self, manager, hyperparameters=None):
        self.manager: MethodManager = manager
        self.hyperparameters = hyperparameters
        self.parse_hyperparameters(hyperparameters)

    def parse_hyperparameters(self, hyperparameters: dict) -> None:
        self.hyperparameters = hyperparameters

        params = hyperparameters.get("AFT")
        self.distance_fn = dist_euclidean

        # threshold offest for picking new values
        offset = params.get("aft_offset")
        if offset is None:
            print("No aft_offset provided, using 0.001")
            self.offset = 0.001
        else:
            self.offset = offset

    def prepare(self, xtrain=None, ytrain=None):
        pass

    def prepare_dataset(self, x: np.ndarray, y: np.ndarray, ds_info) -> None:
        pass

    def explain(self, x: np.ndarray, y: np.ndarray, k: int = 1, constraints: np.ndarray = None, weights: np.ndarray = None, max_dist: float = np.inf, opt_robust: bool = False, min_robust: float = None, return_regions: bool = False) -> np.ndarray:
        '''
        A method for explaining the samples in x by finding the best candidate contrastive examples generated by the model's detectors

        Parameters
        ----------
        x               : an array of samples, dimensions (nsamples, nfeatures)
        y               : an array of labels which correspond to the labels, (nsamples, )

        Returns
        -------
        best_examples : an array of contrastive examples with dimensions (nsamples, nfeatures). Each of final_examples[i] corresponds to
        the best examples which explains x[i] from those suggested by the detectors
        '''

        # key array dimensions
        nsamples = x.shape[0]
        nfeatures = x.shape[1]
        ndetectors = 1

        if self.hyperparameters is None or self.hyperparameters.get("explainer_k") is None:
            k = 1
        else:
            k = self.hyperparameters.get("explainer_k")

        # an array to store the candidates proposed by each detector
        candidate_examples = np.empty(shape=(ndetectors, (nsamples * k), nfeatures))
        # an array to store the distance between each of the candidates and the corresponding sample of x
        candidate_dists = np.empty(shape=(ndetectors, (nsamples * k)))
        # an arry to store the models predicted class for each of the candidates
        candidate_preds = np.empty(shape=(ndetectors, (nsamples * k)))

        # for each detector
        for i in range(ndetectors):
            # get the candidates for this detector
            det_candidates = self.get_candidate_examples(x, y).reshape(((nsamples * k), nfeatures))
            candidate_examples[i] = det_candidates

            # predict and save the class for each candidate using the model
            # temporarilty swapping invalid candiates (rep by [inf, inf, ... , inf]) to zero
            idx_inf = (det_candidates == np.inf).any(axis=1)
            det_candidates[idx_inf] = np.tile(0, (nfeatures,))
            candidate_preds[i] = self.manager.predict(det_candidates)
            det_candidates[idx_inf] = np.tile(np.inf, (nfeatures,))

            # if an example doesn't result in a changed class prediciton, set it to inf
            idx_unchaged_class = candidate_preds[i] == y
            candidate_examples[i][idx_unchaged_class] = np.tile(np.inf, (nfeatures,))

            # compute and save the distance for each candidate
            candidate_dists[i] = self.distance_fn(np.repeat(x, repeats=k, axis=0), det_candidates)

        # find which model suggests the best candidate for each sample
        idx_best_example = np.argmin(candidate_dists, axis=0)

        # select the best examples
        best_examples = np.empty(shape=(nsamples, nfeatures))
        for i in range(x.shape[0]):
            best_examples[i] = candidate_examples[idx_best_example[i]][i]

        # check that all examples return correct class
        idx_inf = (best_examples == np.inf).any(axis=1)
        best_examples[idx_inf] = np.tile(0, (nfeatures,))
        preds = self.manager.predict(best_examples)
        failed_explanation = (preds == y)
        best_examples[failed_explanation] = np.tile(np.inf, x.shape[1])
        best_examples[idx_inf] = np.tile(np.inf, x.shape[1])

        return best_examples

    def get_candidate_examples(self, x, y):
        '''
        A function for getting the best `k` contrastive example candidates for the samples of `x`. The *best* examples are those which result in a change of predicted class and have a minimal change from `x[i]`.

        The offset amount used in contrastive example generation and the distance metric used to select the minimal examples are set by the hyperparameters `rf_difference` and `rf_distance` determined during initialization

        Parameters
        ----------
        x               : an array of samples, dimensions (nsamples, nfeatures)
        y               : an array of labels which correspond to the labels, (nsamples, )

        Returns
        -------
        final_examples : an array of contrastive examples with dimensions (nsamples, k, nfeatures). Each of final_examples[i] corresponds to an array of the best k examples which explain x[i]
        '''
        rf: RandomForest = self.manager.model

        k = 1
        final_examples = np.empty(shape=(x.shape[0], k, x.shape[1]))

        trees = rf.model.estimators_
        all_examples = [[]] * x.shape[0]  # a list to store the array of examples, one for each example

        # for each tree, get an example for each sample in x
        for t in trees:
            # get an array of candidate examples for each instance in x
            helper = TreeContraster(t)
            tree_examples = helper.construct_examples(x, y, self.offset)

            # merge n_sample arrays from this tree with those from the other trees
            for i in range(x.shape[0]):
                if len(all_examples[i]) == 0:
                    all_examples[i] = tree_examples[i]
                else:
                    if (len(tree_examples[i]) > 0):
                        all_examples[i] = np.vstack([all_examples[i], tree_examples[i]])

        # for each sample pick the top k best candidates that result in a changed class
        progress = tqdm(total=x.shape[0], desc="AFT", leave=False)
        for i in range(x.shape[0]):
            # get the info for that sample
            instance = x[i]
            label = y[i]
            candidate_examples = all_examples[i]

            # get the class predicted by the forest for each candidate
            candidate_preds = self.manager.predict(candidate_examples)

            # keep only candidates that result in a changed prediction
            candidate_examples = candidate_examples[candidate_preds != label]

            # sort the candidates according to the distance function
            candidate_dists = self.distance_fn(instance, candidate_examples)
            sort_idxs = np.argsort(candidate_dists)
            candidate_examples = candidate_examples[sort_idxs]
            candidate_preds = candidate_preds[sort_idxs]

            # return the top k candidates for x if availible
            if candidate_examples.shape[0] >= k:
                top_k_candidates = candidate_examples[:k]
            # if there are fewer than k candidates, pad with [inf, inf, ... , inf]
            else:
                n_missing = k - candidate_examples.shape[0]
                pad_candidates = np.tile(np.inf, (n_missing, instance.shape[0]))
                top_k_candidates = np.vstack((candidate_examples, pad_candidates))

            final_examples[i] = top_k_candidates
            progress.update()

        progress.close()
        if k == 1:
            final_examples = final_examples.squeeze()

        return final_examples
